{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e69995a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB client initialized\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.Client()\n",
    "print(\"ChromaDB client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ef0a160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing entity_collection\n",
      "Created new entity_collection with cosine similarity for L2-normalized embeddings\n"
     ]
    }
   ],
   "source": [
    "# Create or get entity collection without default embedding function\n",
    "try:\n",
    "    # Try to delete existing collection if it exists\n",
    "    chroma_client.delete_collection(name=\"entity_collection\")\n",
    "    print(\"Deleted existing entity_collection\")\n",
    "except:\n",
    "    print(\"No existing entity_collection to delete\")\n",
    "\n",
    "# Create new collection WITHOUT embedding function (we'll use pre-computed embeddings)\n",
    "# For L2-normalized vectors, cosine similarity is more appropriate than Euclidean distance\n",
    "entity_collection = chroma_client.create_collection(\n",
    "    name=\"entity_collection\",\n",
    "    metadata={\"description\": \"Collection of medical entities with L2-normalized embeddings and metadata\",\n",
    "              \"hnsw:space\": \"cosine\"},  # Use cosine similarity for normalized vectors\n",
    "    embedding_function=None  # Important: Don't use default embedding function\n",
    ")\n",
    "print(\"Created new entity_collection with cosine similarity for L2-normalized embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f59dfe09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data with 161 chunks\n",
      "Extracted 2516 entities with embeddings\n",
      "Sample entity: surgery\n"
     ]
    }
   ],
   "source": [
    "# Load and extract entity data from entity_processing_results.json\n",
    "with open(\"entity_processing_results_unmc_complete.json\", 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(f\"Loaded data with {len(data.get('chunks', {}))} chunks\")\n",
    "\n",
    "# Extract entities with their embeddings and metadata\n",
    "entities_data = []\n",
    "entity_count = 0\n",
    "\n",
    "for chunk_number, chunk_data in data.get(\"chunks\", {}).items():\n",
    "    for paragraph_idx, paragraph in enumerate(chunk_data.get(\"paragraphs\", [])):\n",
    "        paragraph_content = paragraph.get(\"content\", \"\")\n",
    "        \n",
    "        for entity in paragraph.get(\"entities\", []):\n",
    "            entity_name = entity.get(\"entity_name\", \"\")\n",
    "            entity_embedding = entity.get(\"content_embedding\", [])\n",
    "            entity_description = entity.get(\"entity_description\", \"\")\n",
    "            entity_type = entity.get(\"entity_type\", \"Unknown\")\n",
    "            \n",
    "            # Skip entities without embeddings\n",
    "            if not entity_embedding or len(entity_embedding) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Create unique ID for each entity\n",
    "            entity_id = f\"chunk_{chunk_number}_para_{paragraph_idx}_entity_{entity_count}\"\n",
    "            \n",
    "            entities_data.append({\n",
    "                \"id\": entity_id,\n",
    "                \"entity_name\": entity_name,\n",
    "                \"embedding\": entity_embedding,\n",
    "                \"metadata\": {\n",
    "                    \"chunk_number\": chunk_number,\n",
    "                    \"paragraph_index\": paragraph_idx,\n",
    "                    \"paragraph_content\": paragraph_content,\n",
    "                    \"entity_description\": entity_description,\n",
    "                    \"entity_type\": entity_type\n",
    "                }\n",
    "            })\n",
    "            entity_count += 1\n",
    "\n",
    "print(f\"Extracted {len(entities_data)} entities with embeddings\")\n",
    "print(f\"Sample entity: {entities_data[0]['entity_name'] if entities_data else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b0e6663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 2516 entities to ChromaDB in batches of 1000\n",
      "Added batch 1: entities 1-1000\n",
      "Added batch 2: entities 1001-2000\n",
      "Added batch 3: entities 2001-2516\n",
      "✓ Successfully added 2516 entities to ChromaDB\n",
      "Collection count: 2516\n"
     ]
    }
   ],
   "source": [
    "# Populate ChromaDB collection with entity data\n",
    "if entities_data:\n",
    "    # Prepare data for ChromaDB\n",
    "    ids = [entity[\"id\"] for entity in entities_data]\n",
    "    embeddings = [entity[\"embedding\"] for entity in entities_data]\n",
    "    documents = [entity[\"entity_name\"] for entity in entities_data]  # Use entity name as document text\n",
    "    metadatas = [entity[\"metadata\"] for entity in entities_data]\n",
    "    \n",
    "    # Add to ChromaDB collection in batches (ChromaDB has limits on batch size)\n",
    "    batch_size = 1000\n",
    "    total_entities = len(entities_data)\n",
    "    \n",
    "    print(f\"Adding {total_entities} entities to ChromaDB in batches of {batch_size}\")\n",
    "    \n",
    "    for i in range(0, total_entities, batch_size):\n",
    "        end_idx = min(i + batch_size, total_entities)\n",
    "        batch_ids = ids[i:end_idx]\n",
    "        batch_embeddings = embeddings[i:end_idx]\n",
    "        batch_documents = documents[i:end_idx]\n",
    "        batch_metadatas = metadatas[i:end_idx]\n",
    "        entity_collection.add(\n",
    "            ids=batch_ids,\n",
    "            embeddings=batch_embeddings,\n",
    "            documents=batch_documents,\n",
    "            metadatas=batch_metadatas\n",
    "        )\n",
    "        \n",
    "        print(f\"Added batch {i//batch_size + 1}: entities {i+1}-{end_idx}\")\n",
    "    \n",
    "    print(f\"✓ Successfully added {total_entities} entities to ChromaDB\")\n",
    "    print(f\"Collection count: {entity_collection.count()}\")\n",
    "else:\n",
    "    print(\"No entities found to add to ChromaDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "403ecf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ BioMedBERT model loaded for query embeddings\n",
      "✓ Query embeddings will use the same format as entity embeddings\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import torch\n",
    "\n",
    "    # Load BioMedBERT model for query embeddings\n",
    "    model_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    def generate_query_embedding(text: str, entity_type: str = \"Medical Entity\", description: str = \"\") -> List[float]:\n",
    "        \"\"\"Generate embedding for query text using BioMedBERT with the same format as entity embeddings.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Format the query text to match entity embedding format\n",
    "            # This matches the format used in embedding_pipeline.py's encode_entity method\n",
    "            if description:\n",
    "                formatted_text = f\"entity_name: {text} entity_type: {entity_type} entity_description: {description}\"\n",
    "            else:\n",
    "                # If no description provided, create a simple one\n",
    "                formatted_text = f\"entity_name: {text} entity_type: {entity_type} entity_description: A medical concept related to {text}\"\n",
    "\n",
    "            inputs = tokenizer(formatted_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            # Mean pooling over token embeddings (matching embedding_pipeline.py)\n",
    "            token_embeds = outputs.last_hidden_state\n",
    "            attention_mask = inputs['attention_mask']\n",
    "\n",
    "            # Perform mean pooling\n",
    "            mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeds.size()).float()\n",
    "            sum_embeds = (token_embeds * mask_expanded).sum(dim=1)\n",
    "            sum_mask = mask_expanded.sum(dim=1).clamp(min=1e-9)\n",
    "            mean_pooled = sum_embeds / sum_mask\n",
    "\n",
    "            # L2 normalize (matching embedding_pipeline.py's normalize=True default)\n",
    "            embeddings = torch.nn.functional.normalize(mean_pooled, p=2, dim=1)\n",
    "\n",
    "            return embeddings[0].tolist()\n",
    "\n",
    "    print(\"✓ BioMedBERT model loaded for query embeddings\")\n",
    "    print(\"✓ Query embeddings will use the same format as entity embeddings\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Warning: transformers library not available. Using simple embedding search fallback.\")\n",
    "\n",
    "    # Fallback: search by entity name similarity without embeddings\n",
    "    def generate_query_embedding(text: str, entity_type: str = \"Medical Entity\", description: str = \"\") -> None:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "999cbdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query functionality to find similar entities\n",
    "def query_similar_entities(query_text: str = None, query_embedding: List[float] = None, n_results: int = 5, entity_type: str = None, description: str = None):\n",
    "    \"\"\"\n",
    "    Query the entity collection for similar entities based on embedding       \n",
    "similarity.\n",
    "\n",
    "    Args:\n",
    "        query_text: Text to search for similar entities (optional if\n",
    "query_embedding provided)\n",
    "        query_embedding: Pre-computed embedding vector to search with\n",
    "(optional if query_text provided)\n",
    "        n_results: Number of similar entities to return\n",
    "        entity_type: Optional entity type to provide context (e.g.,\n",
    "\"Disease or Syndrome\", \"Pharmacologic Substance\")\n",
    "        description: Optional description to provide more context about       \n",
    "what you're searching for\n",
    "\n",
    "    Returns:\n",
    "        Query results with entities and their metadata\n",
    "\n",
    "    Examples:\n",
    "        # Search by text\n",
    "        query_similar_entities(\"infection\", n_results=5)\n",
    "\n",
    "        # Search by embedding from entity_processing_results.json\n",
    "        import json\n",
    "        with open('entity_processing_results.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        embedding =\n",
    "data['chunks']['4']['paragraphs'][0]['entities'][0]['content_embedding']      \n",
    "        query_similar_entities(query_embedding=embedding, n_results=5)        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        if query_text is None and query_embedding is None:\n",
    "            raise ValueError(\"Either query_text or query_embedding must be provided\")\n",
    "\n",
    "        if query_text is not None and query_embedding is not None:\n",
    "            print(\"Warning: Both query_text and query_embedding provided. Using query_embedding.\")\n",
    "\n",
    "        # If query_embedding is provided directly, use it\n",
    "        if query_embedding is not None:\n",
    "            # Ensure it's a list and has the right dimension\n",
    "            if not isinstance(query_embedding, list):\n",
    "                query_embedding = list(query_embedding)\n",
    "\n",
    "            # Normalize the embedding if needed (check if already normalized)\n",
    "            import numpy as np\n",
    "            embedding_array = np.array(query_embedding)\n",
    "            norm = np.linalg.norm(embedding_array)\n",
    "\n",
    "            if abs(norm - 1.0) > 0.001:  # Not normalized\n",
    "                print(f\"Normalizing provided embedding (original norm: {norm:.4f})\")\n",
    "                embedding_array = embedding_array / norm\n",
    "                query_embedding = embedding_array.tolist()\n",
    "\n",
    "            # Use the provided embedding for search\n",
    "            results = entity_collection.query(\n",
    "                query_embeddings=[query_embedding],\n",
    "                n_results=n_results\n",
    "            )\n",
    "\n",
    "            print(f\"Query: Using provided embedding (dim={len(query_embedding)})\")\n",
    "\n",
    "        else:\n",
    "            # Generate embedding from text\n",
    "            if entity_type is None:\n",
    "                # Try to infer entity type from query text\n",
    "                if any(word in query_text.lower() for word in ['infection', 'disease', 'syndrome', 'disorder']):\n",
    "                    entity_type = \"Disease or Syndrome\"\n",
    "                elif any(word in query_text.lower() for word in ['drug','medication', 'medicine']):\n",
    "                    entity_type = \"Pharmacologic Substance\"\n",
    "                elif any(word in query_text.lower() for word in ['surgery', 'procedure', 'treatment']):\n",
    "                    entity_type = \"Therapeutic or Preventive Procedure\"       \n",
    "                else:\n",
    "                    entity_type = \"Medical Entity\"\n",
    "\n",
    "            if description is None:\n",
    "                description = f\"A medical concept related to {query_text}\"    \n",
    "\n",
    "            generated_embedding = generate_query_embedding(query_text, entity_type, description)\n",
    "\n",
    "            if generated_embedding is None:\n",
    "                # Fallback: search by document text if embeddings not available\n",
    "                print(\"Using text-based search (embeddings not available)\")\n",
    "                results = entity_collection.query(\n",
    "                    query_texts=[query_text],\n",
    "                    n_results=n_results\n",
    "                )\n",
    "            else:\n",
    "                # Query embedding is already normalized by generate_query_embedding\n",
    "                # Use embedding-based search\n",
    "                results = entity_collection.query(\n",
    "                    query_embeddings=[generated_embedding],\n",
    "                    n_results=n_results\n",
    "                )\n",
    "\n",
    "            print(f\"Query: '{query_text}' (type: {entity_type})\")\n",
    "\n",
    "        print(f\"Found {len(results['ids'][0])} similar entities:\\n\")\n",
    "\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            entity_id = results['ids'][0][i]\n",
    "            entity_name = results['documents'][0][i]\n",
    "            distance = results['distances'][0][i] if 'distances' in results else None\n",
    "            metadata = results['metadatas'][0][i]\n",
    "\n",
    "            print(f\"{i+1}. Entity: {entity_name}\")\n",
    "            if distance is not None:\n",
    "                # Convert distance to similarity score for better interpretation\n",
    "                similarity = 1 - (distance / 2)  # Cosine distance to similarity\n",
    "                print(f\"   Distance: {distance:.4f} (Similarity: {similarity:.2%})\")\n",
    "            print(f\"   Type: {metadata.get('entity_type', 'Unknown')}\")       \n",
    "            print(f\"   Description: {metadata.get('entity_description','N/A')}\")\n",
    "            print(f\"   Chunk: {metadata.get('chunk_number')}, Paragraph: {metadata.get('paragraph_index')}\")\n",
    "            print(f\"   Context: {metadata.get('paragraph_content', 'N/A')[:100]}...\")\n",
    "            print()\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying entities: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "        # Try fallback approach - search by entity names directly\n",
    "        if query_text:\n",
    "            try:\n",
    "                print(\"\\nAttempting fallback search by entity name...\")       \n",
    "                # Get all entities and filter by name similarity\n",
    "                all_results = entity_collection.get(limit=1000)\n",
    "\n",
    "                # Simple text matching\n",
    "                matching_entities = []\n",
    "                query_lower = query_text.lower()\n",
    "\n",
    "                for i, doc in enumerate(all_results['documents']):\n",
    "                    if query_lower in doc.lower():\n",
    "                        matching_entities.append({\n",
    "                            'name': doc,\n",
    "                            'metadata': all_results['metadatas'][i],\n",
    "                            'id': all_results['ids'][i]\n",
    "                        })\n",
    "\n",
    "                print(f\"Found {len(matching_entities)} entities containing '{query_text}':\")\n",
    "                for i, entity in enumerate(matching_entities[:n_results]):    \n",
    "                    print(f\"{i+1}. Entity: {entity['name']}\")\n",
    "                    print(f\"   Type:{entity['metadata'].get('entity_type', 'Unknown')}\")\n",
    "                    print(f\"   Chunk: {entity['metadata'].get('chunk_number')}\")\n",
    "                    print()\n",
    "\n",
    "            except Exception as e2:\n",
    "                print(f\"Fallback search also failed: {e2}\")\n",
    "\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec23094d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Demonstrating Direct Embedding Search ===\n",
      "\n",
      "entity_name_from_mimic: SKIN\n",
      "paragraph_content: SKIN: Warm. Cap refill <2s. No rashes. NEUROLOGIC: AOx3. CN2-12 intact. cogwheel UE b/l. Increased tone in LEs, ___ strength b/l ___. Normal sensation.\n",
      "entity_embedding_from_mimic: [0.004791374318301, -0.0030316247139120004, 0.007956776767969001, -0.0039320266805580005, -0.007750302087515001, -0.00133522436954, 0.010447307489812001, 0.004236557520925, 0.0014395231846710001, -6.563484203070402e-05, -0.0017709594685580002, 0.0008952572243280001, 0.006933584343641001, 0.002469127532094, -0.009277086704969, -0.009595424868166, 0.005558195523917, -0.00013514510646900002, -0.001364765805192, -0.003730770433321, -0.007731267251074001, 0.00047628371976300005, -0.0029457588680080003, 0.01057672034949, -0.031202448531985002, 0.0016329463105640002, -0.00359556870535, 0.004896939266473, 0.001264162128791, -0.010365252383053001, 1.514448376838117e-05, 0.005842664744704001, 0.002936255186796, 0.06831751018762501, 0.002302455017343, -0.007879024371504001, -0.00017191772349100002, 0.003733165096491, -0.013733846135437001, -0.007449472323060001, 0.011279575526714, -0.015088592655956001, 0.004403486847877, 0.004485059995204, 0.006621823180466, 0.012931551784276001, -0.011434639804065, 0.0015025150496510002, 0.009344181977212, 0.005261197220534, -0.007132896222174, -0.007090827450156, 0.011480804532766, -0.008887652307748, -0.005939012859016, -0.001355564105324, -0.0030214458238330003, 0.0013483557850120001, 0.0073600085452190005, -0.00034248706651800003, 0.001268276479095, -0.0008371450239790001, -0.004258413333445001, -0.012788659892976001, -0.0032721424940970003, 0.008523465134203, -0.0028387862257650003, -0.018364563584327, -0.016483265906572002, -0.000667946122121, 0.007512469775974, -0.0018767078872770002, 0.007191217504441001, 0.010404597967863001, -0.014643356204032001, 0.0034084687940770003, 0.006099350284785, -0.0007044002413740001, 0.003588380524888, 0.0032813323196020003, 0.0057955188676710005, -0.007461334578692001, -0.00018025915778700002, -0.0011725884396580001, -0.011936883442103, -0.008013116195797, 0.002437855815514, -0.008357242681086001, -0.00217522168532, 0.0057432805188, 0.0015990835381670002, 0.013392734341323001, -6.458276038756594e-05, -3.298587762401439e-05, -0.004577371291816, 0.0019193271873510001, -0.007840217091143001, -0.002157577779144, -0.008507596328854, -0.0036393036134540003, 0.002116741612553, -0.0051180850714440005, 0.0032870124559840003, -0.002611196134239, 0.007152020465582, 0.005241037812083, 0.008694451302289, 0.008604978211224001, -0.0030232011340550003, -0.008658362552523, -0.007434178143739, -0.003257219679653, -0.012646392919123001, 0.006925597321242, -0.003767615417018, 0.005913489498198, 0.006517411675304, -0.003829114604741, -0.002454949310049, 0.019634511321783, -0.006775000132620001, 0.0016004779608920002, 0.0077246883884070005, 0.003987122327089, -0.004042816348373, 0.0038509978912770002, 0.00069408188574, 0.011577513068914, -0.00400698184967, -0.004270609933882, 0.004674017895013, -0.014468836598098, -0.006263452116400001, 0.0013965131947770001, -0.009344536811113, -0.0020063892006870004, -0.001723649445921, 0.005689548328518, 0.011017653159797, -0.0009314632625310001, 0.007323325611650001, -0.002644696040078, -0.006148155778646001, -0.020319322124123, -0.0067728837020690005, -0.007452016230672, -0.007095616776496, 0.00463070673868, -0.00469229184091, 0.0014908005250610001, -0.005785613786429, 0.0031498339958480003, 0.0007635666988790001, 0.009068137034773001, 0.000197120272787, -0.0007313706446430001, -0.010853271000087001, 0.00030067629995700004, -0.009408389218151, 0.0038839557673780003, -0.008875913918018001, -0.004129025153815, 0.000522131100296, 0.003651597304269, -0.0028990104328840004, 0.0016566457925360001, 0.018812768161296, -0.004069040063768, -0.00848148856312, 0.0028828519862140003, 0.008924127556383001, 0.001288662781007, 0.012698657810688001, -0.006913609802722, -0.0010110849980260002, 0.005804838612675001, -0.0004984558909200001, -0.005513107869774001, -0.011383579112589, 0.002074629534035, -0.01743777282536, -0.022025959566235, -0.0006122216582290001, -0.008461696095764, -0.004442747682332, -0.006548872217535, -0.0035614510998120004, 0.007013398688286001, 0.004754052963107, 0.015632724389433, -0.017855344340205002, 0.008413131348788001, -0.003931925166398001, -0.009637727402150001, -0.006400246173143001, 0.003380782436579, -0.003953320439904, 0.012897429056465001, -0.0008257179870270001, -0.0038596957456320004, 0.016294680535793003, -0.0013487258693200002, 0.00029286785866100004, 0.000483073672512, -0.0014964018482710002, -0.0015097249997770002, -0.0007804136257610001, 0.0054418207146220005, 0.002059770515188, 0.008798736147582, -0.000156674330355, -0.004940549843013, 0.000644452462438, 0.0013176084030410002, 0.0066076340153810005, 0.004344890825450001, -0.010862472467124, -0.004670278169214, 0.008017030544579001, -0.000591718300711, -0.0031693794298910003, -0.009874286130070001, -0.0024478912819170004, -0.00038091684109500004, 0.007645077072083, 0.0069664190523320004, -0.00044576512300400004, -0.000903511478099, -0.003111052094027, -0.020031450316309003, -0.0043587698601180004, -0.023781875148415003, -0.009921763092279, -0.005646319128572001, -0.00090281071607, 0.007105235476046001, 0.00019683872233100001, 0.039874639362096, 0.0005872827605340001, 0.008126620203256, -0.003577444935217, 0.010518632829189, -0.012771683745086, 0.007041686680167001, 0.0011097888927900001, 0.005697524175047, 0.012416338548064001, 0.008488615974783, -0.006394181400537001, 0.0033990028314290002, -0.002789528807625, -0.00199406594038, 0.0018171824049200002, 0.000573353783693, -0.0044084992259740005, 0.010952159762382, -0.0027037984691560004, -0.013458780944347002, 0.002417957410216, -0.0017628171481190002, 0.006052103824913, -0.007106032222509001, 0.014829937368631, -0.009219839237630001, 0.0019040501210830002, -0.016178162768483002, -0.014686172828078001, 0.008186221122741, -0.0020920201204710004, 0.001334320404566, -0.001129925949499, 0.010146339423954001, 0.013377497904002, -0.0036336630582800003, -0.00871827173978, 0.013676511123776, 0.004896236117929001, -0.001270213862881, 0.012134265154600001, -0.005478997714817, 0.00312758795917, 0.013029935769736002, -0.005259080789983, -0.0077913012355560005, -0.012638821266591, 0.008491329848766, -0.003031857311725, -0.000128104278701, -0.0046891430392860005, -0.010929222218692, -0.011139011010527, 0.0005634011467910001, -0.008959442377090001, -0.0052050026133650005, -0.003942114766687, -0.02181652188301, 0.005782014224678, -0.0035881986841550004, 0.0006985958316360001, 0.0019007740775120001, -0.011903173290193001, -0.005402465350925, 0.0008306791423810001, -0.005819408223032001, 0.003929021768271, 0.0038865427486590005, -0.005325917620211, 0.019936865195631003, 0.00807181559503, 0.001219523139297, 0.006172547582536001, 0.012655857019126, -0.0034903299529100004, 0.005375852808356001, -0.0005708759417750001, -0.012020182795822001, 0.002011661650612, -0.006117904558777001, -0.001305086771026, 0.002192105166614, -0.010925281792879, 0.00501256622374, -0.006013764999806001, -0.00578342564404, 0.002706419443711, -0.010411352850496, 0.010222153738141001, 0.003779409918934, -0.009443425573408, 0.0018909573554990002, 0.003421032102778, 0.010106187313795001, -0.006474847439676, -0.002124388469383, -0.047177534550428, -0.002066742628812, 0.012278551235795, 0.001110412413254, 0.013973914086818001, 0.006220343057066, 0.004167512990534, 0.011010531336069001, -0.020222423598170003, 0.001787281362339, -0.002017846098169, -0.0048755980096750005, 0.008047593757510001, 0.004809157922863, -0.0039056476671240005, -0.0061995452269910005, 0.008207418955862, -0.0016860151663420002, -0.0017434401670470001, 0.0007138052023940001, 0.004482438787817, -0.003187551628798, -0.005914899054914001, 0.014253705739974, -6.25022075837478e-05, 0.00042701265192500004, -0.008622643537819, -0.001178245758637, 0.009716837666928001, 0.0032772480044510003, 0.0013487556716420001, -0.0035956902429460005, -0.010073777288198001, -0.007419408299028001, -0.005805273074656001, 0.003096308326348, 0.0016548940911880001, -0.013553276658058002, 0.015744820237159, -0.001334212720394, 0.006975313182920001, 0.0013665396254500001, 0.00283292774111, -0.008303388953208, -0.007805317640304001, -0.006341740954667, 0.010975753888487, -0.005446376744657, 0.002129327971488, -0.006943719927221001, 0.0031272310297930004, -0.010723399929702001, -0.007994088344275, -0.005285404156893001, 0.001848132465966, -0.006999943871051, -0.011245644651353, 0.009292481467127, 0.004774549975991, -0.0010179509408770001, 0.002908062189817, -0.001736264908686, 0.023400224745273, 0.009163383394479, -0.002388947177678, -0.00925996992737, -0.003945806529372001, -0.006569563876837, 0.0026533869095140003, 0.0044583999551830004, -0.0044439076445990005, 0.006939215585589, -0.0007705853204240001, -0.006189840845763001, -0.004929495975375, 0.004047730006277, 0.00231126579456, 0.011570303700864001, 0.00323071796447, 0.001028121449053, 0.01619954034686, -0.0068719531409440005, -0.009464661590754, 0.009254428558051002, 0.000988378655165, -0.0018420001724730002, -0.014436327852308, 0.0027260459028180003, 0.006076261401176001, -0.019308075308799, -0.9660290479660031, -0.004059260245412, 0.0035398043692110004, 0.002686535939574, 0.006991647183895001, 0.0022161391098050004, 0.000527444994077, 0.001977284671738, -0.0014653960242860002, -0.005834406707435001, -0.009325423277914, -0.004920092411339, -0.010816339403390001, -0.00125566648785, -0.015714032575488, 0.008321680128574, 0.014055552892386, -0.004245690535753, -0.014547409489750002, -0.0022362291347230003, 0.011571905575692001, -0.005281161516904, 0.007278578355908001, 0.006995666306465, -0.0025998204946510004, -0.00632822792977, 0.004455112386494001, 0.010821939446032, 0.011366438120603001, -0.006876428611576, 0.005451773758977, 0.000104432998341, -0.010916694067418001, -0.001255296403542, -0.006722063291817001, -0.016483044251799, -0.002220508409664, -0.008900378830730001, -0.006393116898834, -0.003244303865358, -0.00051627587527, -0.000938071811106, -0.009958592243492001, 0.002951037604361, 0.0027695125900200003, -0.009162471629679, -0.087172292172908, -0.045601047575473, 0.0074872272089120005, 0.002252169186249, 0.021012300625443, -0.0015599656617260001, 0.0019703570287670003, -0.011238505132496001, -0.006357391364872, -0.00531463464722, 0.003977410960942, 0.004347363486886, 0.007660069502890001, -0.001004138728603, -0.013535612262785001, -0.007563231512904001, -0.001823968952521, -0.009394852444529, 0.0059451390989120004, 0.001198768150061, 0.014415305107831, 0.004886548034846, 0.0007905721431590001, -0.0032230420038100002, 0.007726891431957, -0.000307356967823, 0.030983258038759003, -0.010716043412685, 0.010560372844338, -0.004812071565538, -0.005426428280770001, 0.0008427177090190001, 0.04884911701083101, -0.002443796955049, 0.004508667625486001, 0.007357512600719001, -0.0034511499106880004, 0.005203858483582, 0.007841636426746, -0.007246328517794, -0.004804250318557001, -0.009131679311394001, 0.016346590593457, -0.002485552104189, -0.0026363271754230003, 0.007293960079550001, -0.008670520968735001, -0.005932380910962, 0.007754999678581001, 0.003083142917603, 0.004820822738111, 0.003721831133589, 0.005512109491974, 0.008862084709107002, -0.012229691259562001, 0.001531155430711, 0.0037461007013910004, 0.000208541867323, -0.005563892424106, 0.0018756890203800003, -0.0008477846859020001, -0.006617575418204001, 0.0029334018472580003, -0.0065075815655290005, 0.0036872683558610004, -0.008046397939324, -0.010139222256839001, 0.000515121093485, -0.0032293240074060003, 0.005546702072024, 0.010724863968789002, 0.0037275403738020003, -0.004150281194597, -0.0010277711553490002, -0.006837606430053001, 0.006647536996752, -0.001643222756683, -0.0017172009684140001, -0.015476777218282, 0.0031775883398950005, -0.016996301710605, -0.005320418160408001, 0.0028070968110110004, 0.004814723040908, -0.0030829391907900004, 0.004773053340613001, 0.0013140902156010001, -0.008867688477039, -0.00955413468182, -0.0023008834104980003, 0.014038076624274, -9.262849403057771e-07, -0.0023268763907250004, 0.00045326293911700005, -0.008581353351473, 0.012007572688162, -0.008016952313482, 0.0058508818037800005, -0.007531762123107, -0.007383399177342, -0.0049594342708580005, 0.0021025435999030002, 0.0042216852307310005, -0.0031297430396080004, 0.005904432851821, -0.006340792868286001, 0.006056489888578, -0.005571897607296, -0.005932703614234001, -0.016254244372248, 0.004155893810093, -0.0034393751993770003, -0.016240928322076003, -0.002764485310763, -0.0007125289994290001, 0.01711687631905, 0.009402140974998, -0.0023817268665870003, 0.005214879289269001, 0.0015656369505450002, 0.0036367103457450004, 0.005179460626095001, 0.013023774139583002, 0.008120165206491, 0.00452656764537, 0.0032456617336720003, 0.001383360824547, 0.005514537915587, -0.002139070536941, -0.001138650695793, -0.0018957112915810002, 0.004014952108263001, -0.004269265569746001, -0.010662485845386, -0.012178285978734, 0.007126488257199, 0.0034078576136380005, 0.013248857110738001, -0.0015112546971060002, -0.011195051483809, 0.001621671952307, 0.00950224045664, -0.000302050029858, -0.008978336118161, -0.002685965737327, -0.006423794664442001, -0.010441343300044, -0.012699452228844001, -0.012585442513227001, 0.004828931763768, -0.002376750577241, 0.006917525082826, -0.001713831326924, -0.009549571201205, -0.010487173683941, -0.0038442418444900005, -0.00029212169465600004, -0.005296012386679001, -0.007665578741580001, -0.010294201783835001, 0.0018906485056500002, 0.043796177953481, -0.004301181528717, 0.0011612602975210001, -0.002947229892015, -0.0013836807338520002, -0.000552876852452, 0.012439374811947, -0.000712769397068, 0.002112382324412, -0.000520364963449, -0.004940688144415, -0.010468012653291002, -0.0059476774185890004, 0.001122273504734, -0.0012944027548650002, -0.005948435049504, 0.000712773995473, 0.0021729632280760003, 0.008694311603903, 0.002595183905214, -7.147268479457125e-05, -0.006070407573133, -0.0021889242343600002, 0.01267984509468, -0.0022487605456260004, 0.004813290201127, -0.013794561848044002, 0.013820286840200001, 0.0036711283028120004, 0.001346715027466, -0.004570074379444, -0.008491075597703, 0.000645057065412, 0.004340187646448001, 0.011729577556252, -0.012097295373678001, 0.000323737360304, -0.005304001271724, 0.010774372145533001, -0.001427248236723, -0.004074278287589, 0.007586837280541001, 0.007805848494172001, -0.001441022497601, 0.003799659432843, 0.004191600717604001, -0.0017751294653860002, -0.009098929353058, -0.0007019415497770001, 0.011239041574299, -0.0036479709669940004, 0.0070909471251070005, 0.0073207635432480005, 8.49845018819906e-05, 0.0038240894209590004, -0.003331247717142, -0.004854905419051, -0.00029509566957100004, -0.00575206708163, -0.005590785294771, 0.005830008070915001, 0.007821772247552001, 0.006205429788678001, 0.0023299183230840004, -0.003186082001775, 0.007448993623256001, 0.013037994503974, 0.0057486705482, -0.006176575552672, -0.004427492152899001, -0.007013415917754001, -0.004114489536732001, 0.0031626394484190004, 0.005306167528033001, -0.0029107243753960003, 0.001049831160344, 0.009298042394220002, -0.006632534787058, 0.00451782438904, 0.002797780558466, 0.012992029078304, -0.014924492686986, -0.0033700733911240003, 0.0016720989951860001, -0.006901304703205001, -0.0026209466159340003, -0.0020319018512960003, 0.005844294093549, 0.006820967420935, 0.004763904958963, 0.022652847692370002, -0.00280127604492, 0.018887301906943002, -0.0031427885405710004, -0.000563323672395, -0.021624671295285003, -0.012670366093516001, -0.00030672331922600003, 0.008383117616176, -0.014444343745708, -0.006900656502693001, 0.008265266194939001, 0.015680503100156, 0.002652833936735, -0.0068632503971450005, -0.00041035836329600004, 0.009898684918880001, 0.00020154540834400002, 0.002126169390976, -0.016479400917887, -0.000185297860298, -0.0035068579018110004, -0.003659025765955, 0.0013574654003600001, 0.0057773687876760006, 0.0058644334785640006, 0.001102372654713, -0.007262623868882, 0.006234225817024001, 0.006491861771792001, -0.008136424235999001, 0.004741583019495, -0.007958865724503, 0.004011529963463, 0.00910407770425, -0.0033537249546490002, 0.005745647009462, -0.0027624834328880002, -0.004171170759946, 0.00282714352943, -0.0027753957547240003, 0.017736416310071, 0.00018065549375, -0.00091923616128, -0.0014489569002760002, 0.0020844831597050003, 0.009275823831558]\n"
     ]
    }
   ],
   "source": [
    "# Example: Search by content embedding directly\n",
    "print(\"=== Demonstrating Direct Embedding Search ===\\n\")\n",
    "\n",
    "# Method 1: Using pandas to access the JSON data\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "df = pd.read_json('entity_processing_results_mimic_complete.json')\n",
    "\n",
    "# Access a specific entity's embedding\n",
    "# For example, get the first entity from chunk 4, paragraph 0\n",
    "chunk = 6\n",
    "paragraph = 0\n",
    "print(\"entity_name_from_mimic:\" , df.chunks[str(chunk)]['paragraphs'][paragraph]['entities'][0]['entity_name']+\"\\nparagraph_content:\", df.chunks[str(chunk)]['paragraphs'][paragraph]['content'])\n",
    "example_embedding = df.chunks[str(chunk)]['paragraphs'][paragraph]['entities'][0]['content_embedding']\n",
    "print(\"entity_embedding_from_mimic:\", df.chunks[str(chunk)]['paragraphs'][paragraph]['entities'][0]['content_embedding'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ae7b188d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Using provided embedding (dim=768)\n",
      "Found 3 similar entities:\n",
      "\n",
      "1. Entity: skin\n",
      "   Distance: 0.0046 (Similarity: 99.77%)\n",
      "   Type: Body System\n",
      "   Description: The outermost layer of the body that provides protection against external factors. In this context, bacteria are also present on the skin and may lead to infections after transplant.\n",
      "   Chunk: 62, Paragraph: 0\n",
      "   Context: Bacterial infections frequently occur after transplant. Bacteria are normally found throughout the b...\n",
      "\n",
      "2. Entity: skin\n",
      "   Distance: 0.0046 (Similarity: 99.77%)\n",
      "   Type: Body System\n",
      "   Description: The outermost layer of the body that provides protection against external factors. In this context, bacteria are also present on the skin and may lead to infections after transplant.\n",
      "   Chunk: 211, Paragraph: 0\n",
      "   Context: Perineal Care - special care of the skin and tissue in the genital and rectal areas....\n",
      "\n",
      "3. Entity: skin\n",
      "   Distance: 0.0046 (Similarity: 99.77%)\n",
      "   Type: Body System\n",
      "   Description: The outermost layer of the body that provides protection against external factors. In this context, bacteria are also present on the skin and may lead to infections after transplant.\n",
      "   Chunk: 211, Paragraph: 2\n",
      "   Context: Petechiae - a small dark purple or red spots under the skin caused by blood leaking out of the vesse...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': [['chunk_62_para_0_entity_671',\n",
       "   'chunk_211_para_0_entity_2393',\n",
       "   'chunk_211_para_2_entity_2415']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['skin', 'skin', 'skin']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'paragraph_content': 'Bacterial infections frequently occur after transplant. Bacteria are normally found throughout the body and on the skin. Normally, these bacteria do not typically cause a problem, however, they may lead to infections after transplant because of the immunosuppressive medications you are',\n",
       "    'entity_type': 'Body System',\n",
       "    'entity_description': 'The outermost layer of the body that provides protection against external factors. In this context, bacteria are also present on the skin and may lead to infections after transplant.',\n",
       "    'paragraph_index': 0,\n",
       "    'chunk_number': '62'},\n",
       "   {'paragraph_index': 0,\n",
       "    'paragraph_content': 'Perineal Care - special care of the skin and tissue in the genital and rectal areas.',\n",
       "    'chunk_number': '211',\n",
       "    'entity_description': 'The outermost layer of the body that provides protection against external factors. In this context, bacteria are also present on the skin and may lead to infections after transplant.',\n",
       "    'entity_type': 'Body System'},\n",
       "   {'entity_description': 'The outermost layer of the body that provides protection against external factors. In this context, bacteria are also present on the skin and may lead to infections after transplant.',\n",
       "    'entity_type': 'Body System',\n",
       "    'paragraph_index': 2,\n",
       "    'chunk_number': '211',\n",
       "    'paragraph_content': 'Petechiae - a small dark purple or red spots under the skin caused by blood leaking out of the vessels; may indicate a low platelet count.'}]],\n",
       " 'distances': [[0.004580795764923096,\n",
       "   0.004580795764923096,\n",
       "   0.004580795764923096]]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_similar_entities(query_embedding=example_embedding, n_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "01aa9255",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File entity_processing_results.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mentity_processing_results.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m df.chunks[\u001b[33m'\u001b[39m\u001b[33m4\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mparagraphs\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mentities\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent_embedding\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\noahb\\miniforge3\\envs\\ep1-venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:791\u001b[39m, in \u001b[36mread_json\u001b[39m\u001b[34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient != \u001b[33m\"\u001b[39m\u001b[33mtable\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    789\u001b[39m     convert_axes = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m json_reader = \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[32m    813\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\noahb\\miniforge3\\envs\\ep1-venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:904\u001b[39m, in \u001b[36mJsonReader.__init__\u001b[39m\u001b[34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[39m\n\u001b[32m    902\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = filepath_or_buffer\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mujson\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28mself\u001b[39m._preprocess_data(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\noahb\\miniforge3\\envs\\ep1-venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:960\u001b[39m, in \u001b[36mJsonReader._get_data_from_filepath\u001b[39m\u001b[34m(self, filepath_or_buffer)\u001b[39m\n\u001b[32m    952\u001b[39m     filepath_or_buffer = \u001b[38;5;28mself\u001b[39m.handles.handle\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    954\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    955\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer.lower().endswith(\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[32m    959\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    962\u001b[39m     warnings.warn(\n\u001b[32m    963\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing literal json to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mread_json\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future version. To read from a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    967\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    968\u001b[39m     )\n",
      "\u001b[31mFileNotFoundError\u001b[39m: File entity_processing_results.json does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('entity_processing_results.json')\n",
    "\n",
    "df.chunks['4']['paragraphs'][0]['entities'][0]['content_embedding']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb452e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Collection Statistics ===\n",
      "Total entities in collection: 378\n",
      "\n",
      "Entity types in sample:\n",
      "  Intellectual Product: 8\n",
      "  Body Part, Organ, or Organ Component: 6\n",
      "  Temporal Concept: 5\n",
      "  Activity: 4\n",
      "  Tissue: 4\n",
      "  Health Care Activity: 4\n",
      "  Social Behavior: 4\n",
      "  Population Group: 4\n",
      "  Idea or Concept: 3\n",
      "  Family Group: 3\n",
      "\n",
      "Chunks represented: 4\n",
      "\n",
      "=== Example Queries ===\n",
      "\n",
      "--- Query: infection ---\n",
      "Query: 'infection'\n",
      "Found 5 similar entities:\n",
      "\n",
      "1. Entity: immunosuppression\n",
      "   Distance: 0.0654\n",
      "   Type: Therapeutic or Preventive Procedure\n",
      "   Description: found in blood to protect from threats\n",
      "   Chunk: 6, Paragraph: 1\n",
      "   Context: Antibodies are found in your blood and they try to protect you from anything they don't recognize as...\n",
      "\n",
      "2. Entity: change\n",
      "   Distance: 0.0667\n",
      "   Type: Therapeutic or Preventive Procedure\n",
      "   Description: drugs taken after transplant\n",
      "   Chunk: 16, Paragraph: 0\n",
      "   Context: Medications play an important role after transplant. Some of them will be taken for the rest of your...\n",
      "\n",
      "3. Entity: reducing\n",
      "   Distance: 0.0679\n",
      "   Type: Therapeutic or Preventive Procedure\n",
      "   Description: methods to prevent rejection\n",
      "   Chunk: 9, Paragraph: 0\n",
      "   Context: Since rejection is caused by your immune system, we call methods to prevent it **immunosuppression.*...\n",
      "\n",
      "4. Entity: MMF\n",
      "   Distance: 0.0679\n",
      "   Type: Therapeutic or Preventive Procedure\n",
      "   Description: also called Myfortic\n",
      "   Chunk: 30, Paragraph: 0\n",
      "   Context: (Cellcept, Myfortic, also called \"MMF\")...\n",
      "\n",
      "5. Entity: Clear communication\n",
      "   Distance: 0.0682\n",
      "   Type: Social Behavior\n",
      "   Description: essential for transplant care\n",
      "   Chunk: 14, Paragraph: 1\n",
      "   Context: With early detection and good medical care, rejection can be brought under control. It is important ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Additional query examples and analysis functions\n",
    "\n",
    "def query_by_entity_type(entity_type: str, n_results: int = 10):\n",
    "    \"\"\"\n",
    "    Find entities of a specific type.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get entities directly without embedding query\n",
    "        all_results = entity_collection.get(\n",
    "            limit=1000,  # Get a large sample\n",
    "            where={\"entity_type\": entity_type}\n",
    "        )\n",
    "        \n",
    "        print(f\"Entities of type '{entity_type}':\")\n",
    "        print(f\"Found {len(all_results['ids'])} entities:\\n\")\n",
    "        \n",
    "        for i in range(min(n_results, len(all_results['ids']))):\n",
    "            entity_name = all_results['documents'][i]\n",
    "            metadata = all_results['metadatas'][i]\n",
    "            \n",
    "            print(f\"{i+1}. {entity_name}\")\n",
    "            print(f\"   Description: {metadata.get('entity_description', 'N/A')}\")\n",
    "            print(f\"   Chunk {metadata.get('chunk_number')}: {metadata.get('paragraph_content', 'N/A')[:100]}...\")\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying by type: {e}\")\n",
    "\n",
    "def get_collection_stats():\n",
    "    \"\"\"\n",
    "    Get statistics about the entity collection.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        total_count = entity_collection.count()\n",
    "        print(f\"Total entities in collection: {total_count}\")\n",
    "        \n",
    "        # Get a sample of entities to show entity types\n",
    "        sample_results = entity_collection.get(limit=min(100, total_count))\n",
    "        \n",
    "        entity_types = {}\n",
    "        chunks = set()\n",
    "        \n",
    "        for metadata in sample_results['metadatas']:\n",
    "            etype = metadata.get('entity_type', 'Unknown')\n",
    "            entity_types[etype] = entity_types.get(etype, 0) + 1\n",
    "            chunks.add(metadata.get('chunk_number'))\n",
    "        \n",
    "        print(f\"\\nEntity types in sample:\")\n",
    "        for etype, count in sorted(entity_types.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "            print(f\"  {etype}: {count}\")\n",
    "            \n",
    "        print(f\"\\nChunks represented: {len(chunks)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting stats: {e}\")\n",
    "\n",
    "# Run analysis\n",
    "print(\"=== Collection Statistics ===\")\n",
    "get_collection_stats()\n",
    "\n",
    "print(\"\\n=== Example Queries ===\")\n",
    "\n",
    "# Example queries using the updated query function\n",
    "queries = ['infection'\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n--- Query: {query} ---\")\n",
    "    query_similar_entities(query, n_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xiyb6bb1vdg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Search by content embedding directly\n",
    "print(\"=== Demonstrating Direct Embedding Search ===\\n\")\n",
    "\n",
    "# Method 1: Using pandas to access the JSON data\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('entity_processing_results.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Access a specific entity's embedding\n",
    "# For example, get the first entity from chunk 4, paragraph 0\n",
    "if '4' in data['chunks']:\n",
    "    chunk_4 = data['chunks']['4']\n",
    "    if chunk_4['paragraphs'] and chunk_4['paragraphs'][0]['entities']:\n",
    "        first_entity = chunk_4['paragraphs'][0]['entities'][0]\n",
    "        entity_name = first_entity['entity_name']\n",
    "        entity_embedding = first_entity['content_embedding']\n",
    "        \n",
    "        print(f\"Using embedding from entity: '{entity_name}'\")\n",
    "        print(f\"Embedding dimension: {len(entity_embedding)}\")\n",
    "        print(f\"\\nSearching for similar entities...\")\n",
    "        \n",
    "        # Search using the embedding\n",
    "        results = query_similar_entities(query_embedding=entity_embedding, n_results=5)\n",
    "        \n",
    "        # The first result should be the entity itself (or very similar)\n",
    "        if results and results['documents'][0][0] == entity_name:\n",
    "            print(f\"\\n✓ Successfully found the same entity as top result!\")\n",
    "    else:\n",
    "        print(\"No entities found in chunk 4, paragraph 0\")\n",
    "else:\n",
    "    print(\"Chunk 4 not found in the data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Method 2: Direct access using dictionary navigation\n",
    "print(\"Alternative method - Direct dictionary access:\\n\")\n",
    "\n",
    "# You can also access embeddings like this:\n",
    "example_embedding = data['chunks']['2']['paragraphs'][0]['entities'][0]['content_embedding']\n",
    "example_name = data['chunks']['2']['paragraphs'][0]['entities'][0]['entity_name']\n",
    "\n",
    "print(f\"Using embedding from: '{example_name}'\")\n",
    "query_similar_entities(query_embedding=example_embedding, n_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097b7991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f058cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          note_id  subject_id   hadm_id note_type  note_seq  \\\n",
      "0  10000032-DS-21    10000032  22595853        DS        21   \n",
      "1  10000032-DS-22    10000032  22841357        DS        22   \n",
      "2  10000032-DS-23    10000032  29079034        DS        23   \n",
      "3  10000032-DS-24    10000032  25742920        DS        24   \n",
      "4  10000084-DS-17    10000084  23052089        DS        17   \n",
      "\n",
      "             charttime            storetime  \\\n",
      "0  2180-05-07 00:00:00  2180-05-09 15:26:00   \n",
      "1  2180-06-27 00:00:00  2180-07-01 10:15:00   \n",
      "2  2180-07-25 00:00:00  2180-07-25 21:42:00   \n",
      "3  2180-08-07 00:00:00  2180-08-10 05:43:00   \n",
      "4  2160-11-25 00:00:00  2160-11-25 15:09:00   \n",
      "\n",
      "                                                text  \n",
      "0   \\nName:  ___                     Unit No:   _...  \n",
      "1   \\nName:  ___                     Unit No:   _...  \n",
      "2   \\nName:  ___                     Unit No:   _...  \n",
      "3   \\nName:  ___                     Unit No:   _...  \n",
      "4   \\nName:  ___                    Unit No:   __...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('note/discharge.csv.gz')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bba390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nName:  ___                     Unit No:   ___\\n \\nAdmission Date:  ___              Discharge Date:   ___\\n \\nDate of Birth:  ___             Sex:   F\\n \\nService: MEDICINE\\n \\nAllergies: \\nPercocet\\n \\nAttending: ___.\\n \\nChief Complaint:\\nabdominal fullness and discomfort\\n \\nMajor Surgical or Invasive Procedure:\\n___ diagnostic paracentesis\\n___ therapeutic paracentesis\\n\\n \\nHistory of Present Illness:\\n___ with HIV on HAART, COPD, HCV cirrhosis complicated by \\nascites and HE admitted with abdominal distention and pain. She \\nwas admitted to ___ for the same symptoms \\nrecently and had 3L fluid removed (no SBP) three days ago and \\nfelt better. Since discharge, her abdomen has become \\nincreasingly distended with pain. This feels similar to prior \\nepisodes of ascites.  \\nHer diuretics were recently decreased on ___ due to worsening \\nhyponatremia 128 and hyperkalemia 5.1. Patient states she has \\nbeen compliant with her HIV and diuretic medications but never \\nfilled out the lactulose prescription. She states she has had \\n___ BMs daily at home. She has had some visual hallucinations \\nand forgetfulness. Her appetite has been poor.  \\nIn the ED, initial vitals were 98.9 88 116/88 18 97% RA. CBC \\nnear baseline, INR 1.4, Na 125, Cr 0.6. AST and ALT mildly above \\nbaseline 182 and 126 and albumin 2.8. Diagnostic para with 225 \\nWBC, 7% PMN, total protein 0.3. UA with few bact, 6 WBC, mod \\nleuk, neg nitr, but contaminated with 6 epi. CXR clear. RUQ US \\nwith no PV thrombus, moderate ascites. She was given ondansetron \\n4mg IV and morphine 2.5mg IV x1 in the ED.  \\nOn the floor, she is feeling improved but still has abdominal \\ndistention and discomfort.  \\nROS: +Abdominal distention and pain. No black/bloody stools. No \\n___ pain or swelling. No fevers or chills. Denies chest pain, \\nnausea, vomiting. No dysuria or frequency. \\n \\nPast Medical History:\\n1. HCV Cirrhosis  \\n2. No history of abnormal Pap smears.  \\n3. She had calcification in her breast, which was removed  \\npreviously and per patient not, it was benign.  \\n4. For HIV disease, she is being followed by Dr. ___ Dr.  \\n___.  \\n5. COPD  \\n6. Past history of smoking.  \\n7. She also had a skin lesion, which was biopsied and showed  \\nskin cancer per patient report and is scheduled for a complete  \\nremoval of the skin lesion in ___ of this year.  \\n8. She also had another lesion in her forehead with purple  \\ndiscoloration. It was biopsied to exclude the possibility of  \\n___'s sarcoma, the results is pending.  \\n9. A 15 mm hypoechoic lesion on her ultrasound on ___  \\nand is being monitored by an MRI.  \\n10. History of dysplasia of anus in ___.  \\n11. Bipolar affective disorder, currently manic, mild, and PTSD. \\n \\n12. History of cocaine and heroin use.  \\n\\n \\nSocial History:\\n___\\nFamily History:\\nShe a total of five siblings, but she is not  talking to most of \\nthem. She only has one brother that she is in  touch with and \\nlives in ___. She is not aware of any known GI or liver \\ndisease in her family.  \\n \\nPhysical Exam:\\nADMISSION PHYSICAL EXAM:  \\nVS: T98.1 105/57 79 20 97RA 44.6kg  \\nGENERAL: Thin chronically ill appearing woman in no acute \\ndistress  \\nHEENT: Sclera anicteric, MMM, no oral lesions  \\nHEART: RRR, normal S1 S2, no murmurs  \\nLUNGS: Clear, no wheezes, rales, or rhonchi  \\nABD: Significant distention with visible veins, bulging flanks, \\nnontender to palpation, tympanitic on percussion, normal bowel \\nsounds  \\nEXT: no ___ edema, 2+ DP and ___ pulses  \\nNEURO: alert and oriented, not confused, no asterixis\\n\\nDISCHARGE PE:\\nVS: T 98.4 BP 95/55 (SBP ___ HR 80 RR 18 O2 95RA  \\nI/O 240/150 this am  \\nGENERAL: Thin chronically ill appearing woman in no acute \\ndistress  \\nHEENT: Sclera anicteric, MMM, no oral lesions  \\nHEART: RRR, normal S1 S2, no murmurs  \\nLUNGS: Clear, no wheezes, rales, or rhonchi  \\nABD: Significant distention with visible veins, bulging flanks, \\nnontender to palpation, tympanitic on percussion, normal bowel \\nsounds  \\nEXT: no ___ edema, 2+ DP and ___ pulses  \\nNEURO: alert and oriented, not confused, no asterixis\\n \\nPertinent Results:\\nLABS ON ADMISSION:\\n___ 04:10PM BLOOD ___ \\n___ Plt ___\\n___ 04:10PM BLOOD ___ \\n___\\n___ 04:10PM BLOOD ___ \\n___\\n___ 04:10PM BLOOD ___ \\n___\\n___ 04:10PM BLOOD ___\\n___ 04:39PM BLOOD ___\\n\\nLABS ON DISCHARGE:\\n___ 05:10AM BLOOD ___ \\n___ Plt ___\\n___ 05:10AM BLOOD ___ ___\\n___ 05:10AM BLOOD ___ \\n___\\n___ 05:10AM BLOOD ___ \\n___\\n___ 05:10AM BLOOD ___\\n\\nMICRO:\\n___ 10:39 pm URINE      Source: ___. \\n\\n                            **FINAL REPORT ___\\n\\n   URINE CULTURE (Final ___: \\n      MIXED BACTERIAL FLORA ( >= 3 COLONY TYPES), CONSISTENT \\nWITH SKIN\\n      AND/OR GENITAL CONTAMINATION. \\n\\n___ 7:00 pm PERITONEAL FLUID      PERITONEAL FLUID. \\n\\n   GRAM STAIN (Final ___: \\n      1+    (<1 per 1000X FIELD):   POLYMORPHONUCLEAR \\nLEUKOCYTES. \\n      NO MICROORGANISMS SEEN. \\n      This is a concentrated smear made by cytospin method, \\nplease refer to\\n      hematology for a quantitative white blood cell count.. \\n\\n   FLUID CULTURE (Final ___:    NO GROWTH. \\n\\n   ANAEROBIC CULTURE (Preliminary):    NO GROWTH.\\n\\n___ 7:00 pm PERITONEAL FLUID      PERITONEAL FLUID. \\n\\n   GRAM STAIN (Final ___: \\n      1+    (<1 per 1000X FIELD):   POLYMORPHONUCLEAR \\nLEUKOCYTES. \\n      NO MICROORGANISMS SEEN. \\n      This is a concentrated smear made by cytospin method, \\nplease refer to\\n      hematology for a quantitative white blood cell count.. \\n\\n   FLUID CULTURE (Final ___:    NO GROWTH. \\n\\n   ANAEROBIC CULTURE (Preliminary):    NO GROWTH. \\n\\nDiagnositc Para:\\n___ 07:00PM ASCITES ___ \\n___\\n___ 07:00PM ASCITES ___\\n\\nIMAGING:\\n___ CXR- No acute cardiopulmonary abnormality.  \\n___ RUQ US-  \\n1. Extremely coarse and nodular liver echotexture consistent \\nwith a history of cirrhosis.  \\n2. Moderate ascites.  \\n3. Patent portal vein.\\n \\nBrief Hospital Course:\\n___ with HIV on HAART, HCV cirrhosis with ascites and HE, h/o \\nIVDU, COPD, bipolar disorder presents with abdominal discomfort \\ndue to ___ ascites.  \\n \\n# ASCITES. Now diuretic refractory given last tap was three days \\nago with 3L removed and she has already built up moderate \\nascites. Infectious workup negative, with CXR clear, UA \\ncontaminated but not grossly positive so will f/u culture, \\ndiagnostic para with only 225 WBC, RUQ US with no PV thrombus. \\nCompliant with diuretics but not following low sodium diet or \\nfluid restriction. Dr. ___ discussed possible TIPS in \\nthe office but due to lung disease, that was on hold pending \\nfurther cardiac evaluation. Diuretics were recently decreased \\ndue to hyponatremia and hyperkalemia. Held spironolactone for \\nnow due to K 5.2 and increased lasix 20 -> 40. No evidence of \\nsevere hyponatremia (Na<120) or renal failure Cr>2.0 to stop \\ndiuretics at present. Diagnostic paracentesis negative for \\ninfection. Ascitic total protein 0.3 so warrants SBP prophylaxis \\n(<1.0) and fortunately already on Bactrim for PCP prophylaxis \\nwhich would be appropriate for SBP ppx also. Patient did admit \\nto eating pizza and some ___ food prior to \\nadmission. She had therapeutic paracentesis with 4.3L removed \\nand received 37.5G albumin IV post procedure. She felt much \\nbetter with resolution of abdominal discomfort. Patient is \\nscheduled for repeat paracentesis as outpatient on ___.  \\n\\n# HEPATIC ENCEPHALOPATHY. History of HE from Hep C cirrhosis. \\nNow with mild encephalopathy (hallucinations and forgetfulness) \\ndue to medication noncompliance, but not acutely encephalopathic \\nand without asterixis on exam. Infectious workup negative thus \\nfar. Continue lactulose 30mL TID and titrate to 3 BMs daily and \\ncontinue rifaximin 550mg BID. \\n \\n# HYPONATREMIA. Na 125 on admission, 128 four days ago, and 135 \\none month ago. Likely due to third spacing from worsening \\nascites and fluid overload. 1.5L fluid restriction, low salt \\ndiet. S/p therapeutic paracentesis with albumin replacement.\\n\\n# CIRRHOSIS, HEPATITIS C. MELD score of 10 and Child's ___ \\nclass B on this admission. Now decompensated due to ascites. \\nHepatitis C genotype IIIB. Dr. ___ starting \\n___ and ___ with patient in clinic and the \\ninsurance process was started by her office. No history of EGD, \\nneeds this as outpatient for varices screening.  \\n \\n# NUTRITION. Unclear if truly compliant with low salt diet. Poor \\noral intake. Low albumin 2.8 on admission. Met with nutrition. \\n \\n# COAGULOPATHY. INR 1.4 four days ago. No evidence of active \\nbleeding. Very mild thrombocytopenia with plts 143.  \\n \\n# HIV. Most recent CD4 173. On HAART. No established ID \\nprovider. Continue Truvada and Isentress, Bactrim DS daily for \\nPCP ___. Needs outpatient ID appointment  \\n\\n# COPD. Stable. States she is on intermittent home O2 for \\ncomfort at night and with abdominal distentiom. Continued home \\nCOPD meds and home O2 as needed \\n\\n**Transitional Issues**\\n- Discontinued spironolactone ___ elevated potassium\\n- Increased furosemide to 40mg daily\\n- Please recheck electrolytes at next visit\\n- Had paracentesis ___ with 4.3 L removed, received 37.5G \\nalbumin\\n- Needs outpatient ID provider\\n- ___ needs more frequent paracentesis\\n \\nMedications on Admission:\\nThe Preadmission Medication list is accurate and complete.\\n1. Albuterol Inhaler 2 PUFF IH Q6H:PRN wheezing, SOB \\n2. ___ (Truvada) 1 TAB PO DAILY \\n3. Furosemide 20 mg PO DAILY \\n4. Raltegravir 400 mg PO BID \\n5. Spironolactone 50 mg PO DAILY \\n6. Acetaminophen 500 mg PO Q6H:PRN pain,fever \\n7. Tiotropium Bromide 1 CAP IH DAILY \\n8. Rifaximin 550 mg PO BID \\n9. Calcium Carbonate 1250 mg PO BID \\n10. Lactulose 15 mL PO TID \\n11. Sulfameth/Trimethoprim DS 1 TAB PO DAILY \\n\\n \\nDischarge Medications:\\n1. Acetaminophen 500 mg PO Q6H:PRN pain,fever \\n2. Albuterol Inhaler 2 PUFF IH Q6H:PRN wheezing, SOB \\n3. Calcium Carbonate 1250 mg PO BID \\n4. ___ (Truvada) 1 TAB PO DAILY \\n5. Furosemide 40 mg PO DAILY \\n6. Lactulose 15 mL PO TID \\n7. Raltegravir 400 mg PO BID \\n8. Rifaximin 550 mg PO BID \\n9. Sulfameth/Trimethoprim DS 1 TAB PO DAILY \\n10. Tiotropium Bromide 1 CAP IH DAILY \\n\\n \\nDischarge Disposition:\\nHome\\n \\nDischarge Diagnosis:\\nPrimary:  diuretic refractory ascites\\nSecondary: HCV cirrhosis, HIV, hyponatremia, COPD\\n\\n \\nDischarge Condition:\\nMental Status: Clear and coherent.\\nLevel of Consciousness: Alert and interactive.\\nActivity Status: Ambulatory - Independent.\\n\\n \\nDischarge Instructions:\\nDear ___,\\n\\n___ was a pleasure to take care of you at ___ \\n___. You were admitted with abdominal fullness and \\npain from your ascites. You had a diagnostic and therapeutic \\nparacentesis with 4.3 L removed. Your spironolactone was \\ndiscontinued because your potassium was high. Your lasix was \\nincreased to 40mg daily. You are scheduled for another \\nparacentesis on ___ prior to your other appointments that day. \\nPlease call tomorrow to find out the time of the paracentesis. \\nPlease continue to follow a low sodium diet and fluid \\nrestriction. You should call your liver doctor or return to the \\nemergency room if you have abdominal pain, fever, chills, \\nconfusion, or other concerning symptoms.\\n\\nSincerely,\\nYour ___ medical team\\n \\nFollowup Instructions:\\n___\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f2bcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ep1-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
